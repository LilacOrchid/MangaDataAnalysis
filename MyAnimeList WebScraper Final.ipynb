{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manga Data Science Project\n",
    "\n",
    "In this Project I will scrape manga data from MyAnimeList to use in a future project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import requests\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Planning\n",
    "The code for multiple functions that will extract and clean the data from the webpage will be defined and tested .\n",
    "\n",
    "Eventually a class with be created will these methods\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get request of myanimelist to get html \n",
    "html = get('https://myanimelist.net/manga/1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Frame Structure\n",
    "The data frame will use the column headings :\n",
    "\n",
    "This data was obtained by looking at the webpage for MAL(MyAnimeList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_head = [\n",
    "  \n",
    " 'English',\n",
    " 'Type',\n",
    " 'Volumes',\n",
    " 'Chapters',\n",
    " 'Status',\n",
    " 'Published',\n",
    " 'Genres',\n",
    " 'Themes',\n",
    " 'Demographic',\n",
    " 'Serialization',\n",
    " 'Authors',\n",
    " 'Score',\n",
    " 'Ranked',\n",
    " 'Popularity',\n",
    " 'Members',\n",
    " 'Favorites']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Beautiful Soup Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html.text,'html.parser') #BS object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate the 'infomation' section of the page \n",
    "The information section that has the relevant statistics are the child of the class = 'spaceit_pad' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = soup.select('.spaceit_pad  ') #information section\n",
    "info = [line.text for line in info] #grab the text \n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove double whitespace and '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [line.replace('  ',' ').replace('\\n','').strip() for line in info]\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns that are not relevant and add columns that are missing\n",
    "\n",
    "Any columns that are missing from the data will be added and have a value of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = tuple(column_head) #tuple of columns needed \n",
    "\n",
    "info = [line for line in info if line.startswith(relevant_columns)] #grab only info that is relevant\n",
    "\n",
    "for i in range(len(relevant_columns)): # if the info is not in the list then add it with a zero value\n",
    "    col = relevant_columns[i]\n",
    "    if not info[i].startswith(col):\n",
    "        word = relevant_columns[i] + ': 0'\n",
    "        info.insert(i,word) \n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rid of multiple whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [re.sub('\\s+',' ',line) for line in info] #spaces removed\n",
    "info = [re.sub('^\\s','',line) for line in info] #start space removed \n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates words from Genre,Themes, and Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave space after the colon to make string spliting easier later\n",
    "for i in [6,7,8,9,10]:\n",
    "    info[i] = info[i].replace(':',': ')\n",
    "    \n",
    "#Then get rid of duplicate Genre Themes and Demographic    \n",
    "for i in [6,7,8]:\n",
    "    info[i] = info[i].replace(',','') #Get rid of commas\n",
    "    info[i] = ' '.join(dict.fromkeys(info[i].split())) \n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of duplicate words a dictionary is created with keys equal to the words in the string that have been seperated by a white space , then the dictionary keys are joined together using whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the score column\n",
    "The score that is seen from the html.text is not the correct score, the last digit of the float must be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    info[11] = (re.findall('.+\\d+\\.\\d+',info[11])[0])[:-1] #remove the words from the score column\n",
    "except: #If there is no score\n",
    "    info[11] = 'Score: N/A'\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the Ranked column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words from ranked column and last digit \n",
    "try:\n",
    "    info[12] = (re.findall('^Ranked:\\s\\D\\d+',info[12]))[0][:-1]  \n",
    "except: #if there is no rank\n",
    "    info[12] = 'Ranked: N/A'\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Date has now been cleaned\n",
    "The data must now be converted into a dictionary so it can be turned into a pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_row = {}  # dictionary of the row that will be added to DF\n",
    "\n",
    "for line in info: #split keys and value with colon\n",
    "    i = line.split(': ')\n",
    "    add_row[i[0]] = i[1]\n",
    "\n",
    "add_row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show in Pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( columns = column_head, dtype = str) #Create Empty Data frame with columns\n",
    "df_new_row = pd.DataFrame([add_row]) # row that will be added \n",
    "df = pd.concat([df, df_new_row]) # concat the data frames\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation\n",
    "Now I have succesfully scraped the data of 1 webpage into the data frame , now to start the process of automating it for multiple pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitering Request Time\n",
    "If the request time is not monitered an IP ban might take place due to inhuman amounts of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time =time()\n",
    "request = 0\n",
    "\n",
    "for _ in range(5):\n",
    "    request += 1\n",
    "    sleep(randint(1,3))\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(request,request/elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empty Data Frame\n",
    "The Final Data Frame where the manga infomation will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Data Frame\n",
    "df = pd.DataFrame( columns = column_head, dtype = str) #Create Empty Data frame with columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL Codes List\n",
    "The URL for manga is 'https://myanimelist.net/manga/' followed by a numeric code .I will scrape the first 10,000 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_NO = np.arange(7500,8500,1) #list from 1 to 5001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Creation\n",
    "\n",
    "Now I will take the previous data cleaning functions and create a class called AnimeData which will take the URL paramter and have methods to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeData:\n",
    "    \n",
    "    \n",
    "    def __init__(self,link):\n",
    "        self.html = get(link)\n",
    "        self.info = BeautifulSoup((self.html).text,'html.parser')\n",
    "        self.column_head = [\n",
    "                      'English',\n",
    "                     'Type',\n",
    "                     'Volumes',\n",
    "                     'Chapters',\n",
    "                     'Status',\n",
    "                     'Published',\n",
    "                     'Genres',\n",
    "                     'Themes',\n",
    "                     'Demographic',\n",
    "                     'Serialization',\n",
    "                     'Authors',\n",
    "                     'Score',\n",
    "                     'Ranked',\n",
    "                     'Popularity',\n",
    "                     'Members',\n",
    "                     'Favorites']\n",
    "        \n",
    "        self.relevant_columns = tuple(self.column_head)\n",
    "        self.EmptyDataFrame = pd.DataFrame( columns = self.column_head, dtype = str) #Create Empty Data frame with columns \n",
    "        \n",
    "        \n",
    "    def RelevantInfo(self):\n",
    "        '''Extract Infomation statistics from the webpage'''\n",
    "        \n",
    "        self.info = self.info.select('.spaceit_pad  ') #information section\n",
    "        self.info = [line.text for line in self.info] #grab the text \n",
    "        \n",
    "        \n",
    "    def RemoveNewLine(self):\n",
    "        self.info = [line.replace('  ',' ').replace('\\n','').strip() for line in self.info]\n",
    "        \n",
    "        \n",
    "    def RelevantColumns(self):\n",
    "        '''Check if all the relevant statistics are there\n",
    "        if not add the statistic with a value of 0'''\n",
    "        \n",
    "        relevant_columns = tuple(self.column_head) \n",
    "\n",
    "        self.info = [line for line in self.info if line.startswith(relevant_columns)] #grab only info that is relevant\n",
    "\n",
    "        for i in range(len(relevant_columns)): # if the info is not in the list then add it with a zero value\n",
    "            col = str(relevant_columns[i])\n",
    "            if not self.info[i].startswith(col):\n",
    "                word = relevant_columns[i] + ': 0'\n",
    "                self.info.insert(i,word) \n",
    "\n",
    "\n",
    "    def RemoveWhitespace(self):\n",
    "        self.info = [re.sub('\\s+',' ',line) for line in self.info] #spaces removed\n",
    "        self.info = [re.sub('^\\s','',line) for line in self.info] #start space removed \n",
    "        \n",
    "        \n",
    "    def RemoveDuplicateWords(self):\n",
    "        for i in [6,7,8,9,10]:\n",
    "            self.info[i] = self.info[i].replace(':',': ')\n",
    "            \n",
    "\n",
    "        for i in [6,7,8]:\n",
    "            self.info[i] = self.info[i].replace(',','')\n",
    "            self.info[i] = ' '.join(dict.fromkeys(self.info[i].split())) \n",
    "        \n",
    "        \n",
    "            \n",
    "    def CleanScore(self):\n",
    "        '''Remove last digit from score, if there is no score , Score = N/A'''\n",
    "        \n",
    "        try:\n",
    "            self.info[11] = (re.findall('.+\\d+\\.\\d+',info[11])[0])[:-1] #remove the words from the score column\n",
    "        except:\n",
    "            self.info[11] = 'Score: N/A'\n",
    "        \n",
    "        \n",
    "            \n",
    "    def CleanRank(self):\n",
    "        '''Extract only the Ranked value from the string'''\n",
    "        \n",
    "        try:\n",
    "            self.info[12] = (re.findall('^Ranked:\\s\\D\\d+',info[12]))[0][:-1]  #remove words from ranked column and last digit \n",
    "        except:\n",
    "            self.info[12] = 'Ranked: N/A'\n",
    "            \n",
    "        \n",
    "    def AddtoDF(self,DF):\n",
    "        '''Take the infomation statistics, convert it to a DataFrame \n",
    "        then concatenate the DataFrame with the FinalData frame'''\n",
    "        \n",
    "        add_row = {}  # dictionary of the row that will be added to DF\n",
    "\n",
    "        for line in self.info:\n",
    "            i = line.split(': ')\n",
    "            add_row[i[0]] = i[1]\n",
    "        \n",
    "       \n",
    "        df_new_row = pd.DataFrame([add_row]) # row that will be added \n",
    "        FinalDataFrame = pd.concat([DF, df_new_row]) # concat the data frames\n",
    "        return FinalDataFrame\n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Final Data Frame of all the data that is being collected must be defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalDataFrame = pd.DataFrame( columns = column_head, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "request = 0\n",
    "\n",
    "    \n",
    "for code in URL_NO:\n",
    "    link = 'https://myanimelist.net/manga/'+str(code)\n",
    "    manga = AnimeData(link)\n",
    "    \n",
    "    #limit time in between requests\n",
    "    sleep(randint(8,15))\n",
    "        \n",
    "    #monitor the request\n",
    "    request += 1\n",
    "    sleep(randint(1,3))\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s code: {}'.format(request,request/elapsed_time,code))\n",
    "    \n",
    "    if get(link).status_code == 200:  #OK Success Status\n",
    "        manga.RelevantInfo()\n",
    "        manga.RemoveNewLine()\n",
    "        manga.RelevantColumns()\n",
    "        manga.RemoveWhitespace()\n",
    "        manga.RemoveDuplicateWords()\n",
    "        manga.CleanScore()\n",
    "        manga.CleanRank()\n",
    "        FinalDataFrame = manga.AddtoDF(FinalDataFrame)\n",
    "        \n",
    "        \n",
    "    else: #Webpage not found\n",
    "        print(f' html status : {get(link).status_code} ; Code : {code} ; does not exist')\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data Frame as CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinalDataFrame.to_csv('FourthAnime_Data.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
